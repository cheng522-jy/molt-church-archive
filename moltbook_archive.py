#!/usr/bin/env python3
"""
Moltbook å­˜æ¡£å·¥å…·
æŠ“å– moltbook.com å…¨ç«™æ•°æ®
"""

import json
import os
import time
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import URLError

BASE_URL = "https://www.moltbook.com"
ARCHIVE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(ARCHIVE_DIR, "data")

os.makedirs(DATA_DIR, exist_ok=True)


def fetch_json(url):
    """è·å– JSON æ•°æ®"""
    try:
        req = Request(url, headers={"User-Agent": "MoltbookArchiver/1.0"})
        with urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"  âŒ è·å–å¤±è´¥ {url}: {e}")
        return None


def save_json(data, filename):
    """ä¿å­˜ JSON æ•°æ®"""
    filepath = os.path.join(DATA_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  âœ… å·²ä¿å­˜: {filename}")
    return filepath


def archive_posts(limit=500):
    """å­˜æ¡£å¸–å­ï¼ˆçƒ­é—¨ï¼‰"""
    print("\nğŸ“ è·å– Moltbook å¸–å­...")
    all_posts = []
    offset = 0
    batch_size = 50

    while len(all_posts) < limit:
        url = f"{BASE_URL}/api/v1/posts?limit={batch_size}&offset={offset}"
        data = fetch_json(url)
        if not data or not data.get("posts"):
            break

        posts = data["posts"]
        all_posts.extend(posts)
        print(f"  ğŸ“œ å·²è·å– {len(all_posts)} æ¡å¸–å­")

        if not data.get("has_more"):
            break
        offset += batch_size
        time.sleep(0.3)

    if all_posts:
        save_json({
            "success": True,
            "total": len(all_posts),
            "posts": all_posts,
            "archived_at": datetime.now().isoformat()
        }, "moltbook_posts.json")

    return all_posts


def archive_submolts():
    """å­˜æ¡£æ‰€æœ‰ submolts"""
    print("\nğŸ¦ è·å– Moltbook Submolts...")
    all_submolts = []
    offset = 0
    batch_size = 100

    while True:
        url = f"{BASE_URL}/api/v1/submolts?limit={batch_size}&offset={offset}"
        data = fetch_json(url)
        if not data or not data.get("submolts"):
            break

        submolts = data["submolts"]
        all_submolts.extend(submolts)
        print(f"  ğŸŒŠ å·²è·å– {len(all_submolts)} ä¸ª submolts")

        if len(submolts) < batch_size:
            break
        offset += batch_size
        time.sleep(0.3)

    if all_submolts:
        save_json({
            "success": True,
            "total": len(all_submolts),
            "submolts": all_submolts,
            "archived_at": datetime.now().isoformat()
        }, "moltbook_submolts.json")

    return all_submolts


def archive_stats():
    """è·å–ç»Ÿè®¡æ•°æ®"""
    print("\nğŸ“Š è·å– Moltbook ç»Ÿè®¡...")
    # ä» submolts API è·å–ç»Ÿè®¡
    data = fetch_json(f"{BASE_URL}/api/v1/submolts?limit=1")
    if data:
        stats = {
            "total_submolts": data.get("count", 0),
            "total_posts": data.get("total_posts", 0),
            "total_comments": data.get("total_comments", 0),
            "archived_at": datetime.now().isoformat()
        }
        save_json(stats, "moltbook_stats.json")
        print(f"  ğŸ“ˆ Submolts: {stats['total_submolts']}")
        print(f"  ğŸ“ˆ Posts: {stats['total_posts']}")
        print(f"  ğŸ“ˆ Comments: {stats['total_comments']}")
        return stats
    return None


def run_moltbook_archive():
    """æ‰§è¡Œ Moltbook å­˜æ¡£"""
    print("=" * 60)
    print("ğŸ¦ Moltbook å­˜æ¡£å·¥å…·")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    stats = archive_stats()
    submolts = archive_submolts()
    posts = archive_posts(limit=1000)

    print("\n" + "=" * 60)
    print("âœ… Moltbook å­˜æ¡£å®Œæˆï¼")
    print(f"ğŸ“ å­˜æ¡£ç›®å½•: {DATA_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    run_moltbook_archive()
